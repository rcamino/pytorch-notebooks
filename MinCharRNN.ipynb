{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal character RNN\n",
    "\n",
    "![Character sequence](images/charseq.jpeg)\n",
    "\n",
    "Related paper by Andrej Karpathy: [Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. \"Visualizing and understanding recurrent networks.\" arXiv preprint arXiv:1506.02078 (2015).](https://arxiv.org/abs/1506.02078)\n",
    "\n",
    "Related blogpost by Andrej Karpathy: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "Original code by Andrej Karpathy: [gist](https://gist.github.com/karpathy/d4dee566867f8291f086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "A Shakespeare sample can be downloaded from [here](https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tinyshakespeare.txt\", \"r\") as data_file:\n",
    "    data = data_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the amount of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "print(\"Number of symbols in text:\", data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an alphabet from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = set(data)\n",
    "alphabet_size = len(alphabet)\n",
    "print(\"Alphabet size:\", alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a number to every symbol in the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_id = {}\n",
    "id_to_symbol = {}\n",
    "for symbol_id, symbol in enumerate(sorted(alphabet)):\n",
    "    symbol_to_id[symbol] = symbol_id\n",
    "    id_to_symbol[symbol_id] = symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a symbol into a one-hot-encoded vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(symbol):\n",
    "    one_hot_encoded = torch.zeros(alphabet_size)\n",
    "    symbol_id = symbol_to_id[symbol]\n",
    "    one_hot_encoded[symbol_id] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a sequence of symbols into a one-dimensional tensor of symbol IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_tensor(symbols):\n",
    "    return torch.Tensor([symbol_to_id[symbol] for symbol in symbols]).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "\n",
    "class MinCharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MinCharRNN, self).__init__()\n",
    "        \n",
    "        self.input_to_hidden = nn.Linear(alphabet_size, hidden_size)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, alphabet_size)\n",
    "\n",
    "    def forward(self, input_symbol, hidden_state):\n",
    "        hidden_state = torch.tanh(self.input_to_hidden(input_symbol) + self.hidden_to_hidden(hidden_state))\n",
    "        output = self.hidden_to_output(hidden_state)\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to initialize every module (layer) of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight, -0.01, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, the loss funcion and the optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "\n",
    "model = MinCharRNN()    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to load a previously saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"models/min-char-rnn.torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to print a sample text with a fixed amount of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "first_symbol = \"\\n\"\n",
    "symbol_ids = list(range(alphabet_size))\n",
    "\n",
    "def print_sample():\n",
    "    sample = \"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        v_input_symbol = Variable(one_hot_encoding(first_symbol))\n",
    "        v_hidden_state = Variable(torch.zeros((1, hidden_size)))    \n",
    "\n",
    "        for sample_id in range(sample_size):\n",
    "            v_logits, v_hidden_state = model(v_input_symbol, v_hidden_state)\n",
    "\n",
    "            v_probabilities = F.softmax(v_logits, dim=1)\n",
    "            probabilities = v_probabilities.data.squeeze(0).numpy()\n",
    "\n",
    "            symbol_id = np.random.choice(symbol_ids, p=probabilities)\n",
    "            symbol = id_to_symbol[symbol_id]\n",
    "            sample += symbol\n",
    "\n",
    "            v_input_symbol = Variable(one_hot_encoding(symbol))\n",
    "\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial sample without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "sequence_size = 25\n",
    "batches = data_size // (sequence_size + 1)\n",
    "gradient_clipping = 5\n",
    "\n",
    "initial_state = torch.zeros((1, hidden_size))\n",
    "\n",
    "for epoch_id in range(epochs):\n",
    "    # reset the state before every epoch\n",
    "    last_hidden_state = initial_state\n",
    "    \n",
    "    epoch_accumulated_loss = 0.0\n",
    "    \n",
    "    # train\n",
    "    model.train(mode=True)\n",
    "    \n",
    "    with tqdm(total=batches) as progress_bar:\n",
    "        for batch_id in range(batches):\n",
    "            batch_start = batch_id * sequence_size\n",
    "\n",
    "            # reuse the hidden state from last batch\n",
    "            hidden_state = Variable(last_hidden_state)\n",
    "\n",
    "            # clear the gradient information from the past batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # for every symbol in the batch\n",
    "            # try predict the next symbol\n",
    "            # and meassure the loss\n",
    "            predictions = []\n",
    "            for sequence_id in range(sequence_size):\n",
    "                v_input_symbol = Variable(one_hot_encoding(data[batch_start + sequence_id]))\n",
    "\n",
    "                v_prediction, hidden_state = model(v_input_symbol, hidden_state)\n",
    "\n",
    "                predictions.append(v_prediction)\n",
    "\n",
    "            # create all labels\n",
    "            v_labels = Variable(labels_tensor(data[batch_start + 1:batch_start + sequence_size + 1]))\n",
    "\n",
    "            # create all predictions\n",
    "            v_predictions = torch.cat(predictions)\n",
    "\n",
    "            # backpropagate through time\n",
    "            v_loss = loss_function(v_predictions, v_labels)\n",
    "            v_loss.backward()\n",
    "\n",
    "            # gradient clipping to avoid exploding gradients\n",
    "            for parameter in model.parameters():\n",
    "                parameter.grad.data.clamp_(-gradient_clipping, gradient_clipping)\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # batch logging\n",
    "            loss = v_loss.data.item()\n",
    "            epoch_accumulated_loss += loss\n",
    "            \n",
    "            progress_bar.set_postfix(loss=\"{:.03f}\".format(loss))\n",
    "            progress_bar.update()\n",
    "\n",
    "            # take the hidden state out of the variable\n",
    "            # to avoid backpropagating the next batch to this one\n",
    "            last_hidden_state = hidden_state.data\n",
    "    \n",
    "    # epoch logging\n",
    "    mean_loss = epoch_accumulated_loss / float(batches)\n",
    "    print(\"Epoch {:d}/{:d} Mean Loss: {:.03f} Sample:\".format(epoch_id + 1, epochs, mean_loss))\n",
    "    print()\n",
    "    model.train(mode=False)\n",
    "    print_sample()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"models/min-char-rnn.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
