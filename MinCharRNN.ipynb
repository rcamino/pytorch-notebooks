{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal character RNN\n",
    "\n",
    "![Character sequence](images/charseq.jpeg)\n",
    "\n",
    "Related paper by Andrej Karpathy: [Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. \"Visualizing and understanding recurrent networks.\" arXiv preprint arXiv:1506.02078 (2015).](https://arxiv.org/abs/1506.02078)\n",
    "\n",
    "Related blogpost by Andrej Karpathy: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "Original code by Andrej Karpathy: [gist](https://gist.github.com/karpathy/d4dee566867f8291f086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "A Shakespeare sample can be downloaded from [here](https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tinyshakespeare.txt\", \"r\") as data_file:\n",
    "    data = data_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the amount of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of symbols in text: 1115394\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "print(\"Number of symbols in text:\", data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an alphabet from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet size: 65\n"
     ]
    }
   ],
   "source": [
    "alphabet = set(data)\n",
    "alphabet_size = len(alphabet)\n",
    "print(\"Alphabet size:\", alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a number to every symbol in the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_id = {}\n",
    "id_to_symbol = {}\n",
    "for symbol_id, symbol in enumerate(sorted(alphabet)):\n",
    "    symbol_to_id[symbol] = symbol_id\n",
    "    id_to_symbol[symbol_id] = symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a symbol into a one-hot-encoded vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(symbol):\n",
    "    one_hot_encoded = torch.zeros(alphabet_size)\n",
    "    symbol_id = symbol_to_id[symbol]\n",
    "    one_hot_encoded[symbol_id] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a sequence of symbols into a one-dimensional tensor of symbol IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_tensor(symbols):\n",
    "    return torch.Tensor([symbol_to_id[symbol] for symbol in symbols]).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "\n",
    "class MinCharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MinCharRNN, self).__init__()\n",
    "        \n",
    "        self.input_to_hidden = nn.Linear(alphabet_size, hidden_size)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, alphabet_size)\n",
    "\n",
    "    def forward(self, input_symbol, hidden_state):\n",
    "        hidden_state = torch.tanh(self.input_to_hidden(input_symbol) + self.hidden_to_hidden(hidden_state))\n",
    "        output = self.hidden_to_output(hidden_state)\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to initialize every module (layer) of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight, -0.01, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, the loss funcion and the optimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinCharRNN(\n",
       "  (input_to_hidden): Linear(in_features=65, out_features=100, bias=True)\n",
       "  (hidden_to_hidden): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden_to_output): Linear(in_features=100, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "\n",
    "model = MinCharRNN()    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to load a previously saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"models/min-char-rnn.torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to print a sample text with a fixed amount of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "first_symbol = \"\\n\"\n",
    "symbol_ids = list(range(alphabet_size))\n",
    "\n",
    "def print_sample():\n",
    "    sample = \"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        v_input_symbol = Variable(one_hot_encoding(first_symbol))\n",
    "        v_hidden_state = Variable(torch.zeros((1, hidden_size)))    \n",
    "\n",
    "        for sample_id in range(sample_size):\n",
    "            v_logits, v_hidden_state = model(v_input_symbol, v_hidden_state)\n",
    "\n",
    "            v_probabilities = F.softmax(v_logits, dim=1)\n",
    "            probabilities = v_probabilities.data.squeeze(0).numpy()\n",
    "\n",
    "            symbol_id = np.random.choice(symbol_ids, p=probabilities)\n",
    "            symbol = id_to_symbol[symbol_id]\n",
    "            sample += symbol\n",
    "\n",
    "            v_input_symbol = Variable(one_hot_encoding(symbol))\n",
    "\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial sample without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uFL!mFkGTuI-rSu&m?oM!oagzfUI,RQcprivDhNIa\n",
      "TwxD&abkWEm&CcnQH&baScigYEGlmeuWIQdxY3MX:INIWDrO'C.:Vx':it' Yd&kPQbWFnXD,ECx q\n",
      "-pq-rumlB&BmMkQqYJwUe!j$X'MdgtMXZ3$Zn-zlg3.R3\n",
      "PKMNeAPYuxVzVQPfy$?!HxmGq$sNjC3Q.\n"
     ]
    }
   ],
   "source": [
    "print_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [06:56<00:00, 103.01it/s, loss=1.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Mean Loss: 2.167 Sample:\n",
      "\n",
      "Thy\n",
      "Whad\n",
      "CUCNIS\n",
      "S\n",
      "ENANNANNONOUNUMUSHUCUNUCANRUN\n",
      "NANANTANENENICENWENS:\n",
      "MIOMHIIMICHCOZWUCHIMUCUNUSBUMICUCUKUINUCUNUNCINUNCCENCUNUNANCIXISNNWANUNUS\n",
      "HUCUNUNUYUCUCUSTHUCANENDUNUHICSoEThINNCUNUNANHHMICUIS\n",
      "N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [06:35<00:00, 108.55it/s, loss=1.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Mean Loss: 2.016 Sample:\n",
      "\n",
      "Se\n",
      "And then we gint?\n",
      "\n",
      "PONTHOCTANDA:\n",
      "Beris bodnoun, and doy a my be waithhich he flony ma som my cir, gofartet is gledce.\n",
      "\n",
      "PETRUCICENCENTO:\n",
      "Bomeve ape for to bele notP-are,\n",
      "on and Lracted, he he eeds t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [06:38<00:00, 107.77it/s, loss=1.562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Mean Loss: 1.924 Sample:\n",
      "\n",
      "TINAND:\n",
      "Ke,\n",
      "\n",
      "GRENIO:\n",
      "Jio? bestemy the blutaife fecrotire.\n",
      "\n",
      "PETCIO:\n",
      "Gadstardens nouste,\n",
      "And sight not?\n",
      "\n",
      "GREMIONR:\n",
      "Hot thy apeabt.\n",
      "\n",
      "PETRUSIET:\n",
      "Becaist seatesce,\n",
      "Hobantes.\n",
      "\n",
      "ARGCETLAN:\n",
      "Ifaverys!\n",
      "\n",
      "KAMly so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [06:45<00:00, 105.88it/s, loss=1.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Mean Loss: 1.884 Sample:\n",
      "\n",
      "SINA:\n",
      "What I bet:\n",
      "How stoUNE::\n",
      "VINGBAPUENIO:\n",
      "With how incor wer Karath lover;\n",
      "Frion repe: take with the be! it feyst be.\n",
      "\n",
      "BIONTENTIO:\n",
      "I gies; live, wefe:\n",
      "Mate guingicn thid this rever this slawn;\n",
      "Or h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:31<00:00, 95.00it/s, loss=1.494] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Mean Loss: 1.857 Sample:\n",
      "\n",
      "TESCD:\n",
      "He mofnennbut\n",
      "Thats heavus willo.\n",
      "\n",
      "PELTINO:\n",
      "Not to forbinio, and you daipes havit toundiendient noure;\n",
      "And bid matter to. Serpyce!\n",
      "\n",
      "PATHISAY LANDANANIVA:\n",
      "Serfor me we smany, then shat, the Lord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:30<00:00, 95.29it/s, loss=1.469] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Mean Loss: 1.836 Sample:\n",
      "\n",
      "STALY:\n",
      "I canother, to stild this to mone a thoughe havon, neintio staech the now we tang, in san!\n",
      "\n",
      "VINTENCENTIO:\n",
      "Loty?\n",
      "Wear,\n",
      "By be ta-mime sera wilgwert, Wan the cley,\n",
      "Of ishading to a and ate you wit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:28<00:00, 95.59it/s, loss=1.431] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Mean Loss: 1.821 Sample:\n",
      "\n",
      "\n",
      "RIMEN:\n",
      "O: Karint a frupon hath me's ending!\n",
      "\n",
      "KATHANCHAN:\n",
      "Hey?\n",
      "Oss faps, to war.\n",
      "'sware up the wicrould of this\n",
      "of weld lite way, How staw--Hath Pllover\n",
      "Tore it them.\n",
      "\n",
      "ANANIO:\n",
      "Hede in o him am buir on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:11<00:00, 99.48it/s, loss=1.403] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Mean Loss: 1.809 Sample:\n",
      "\n",
      "Se\n",
      "Tast and and prond sirramertiol?\n",
      "\n",
      "NONTENTINA:\n",
      "Busted me, that's to be could ceep on not,\n",
      "You, no devile bestest, allat, astord, boincter?\n",
      "And wifpasteet! the uik, him!\n",
      "\n",
      "PETRUCHIO:\n",
      "And for he daster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:23<00:00, 93.89it/s, loss=1.380] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Mean Loss: 1.799 Sample:\n",
      "\n",
      "TER:\n",
      "What arry-winds\n",
      "you! how for siome have to hold's.\n",
      "\n",
      "PETRUCHIO:\n",
      "Your all.\n",
      "\n",
      "LERNIO:\n",
      "And is ssole,\n",
      "Thus lords! and me the right smand go I will, she somparis: which sore whire you madoth?\n",
      "\n",
      "BAPTISTA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42899/42899 [07:11<00:00, 99.42it/s, loss=1.365] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Mean Loss: 1.791 Sample:\n",
      "\n",
      "SINR:\n",
      "But, kind wive him now thear.\n",
      "\n",
      "VhIO:\n",
      "O to manter have,\n",
      "Womuster'd a many\n",
      "If a have not the stame ont viuen ased?\n",
      "\n",
      "PRINTINCA:\n",
      "O jed.\n",
      "\n",
      "KATHARINCUS:\n",
      "Lere be cangy lower, and re thinss. 'tcunt know \n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "sequence_size = 25\n",
    "batches = data_size // (sequence_size + 1)\n",
    "gradient_clipping = 5\n",
    "\n",
    "initial_state = torch.zeros((1, hidden_size))\n",
    "\n",
    "for epoch_id in range(epochs):\n",
    "    # reset the state before every epoch\n",
    "    last_hidden_state = initial_state\n",
    "    \n",
    "    epoch_accumulated_loss = 0.0\n",
    "    \n",
    "    # train\n",
    "    model.train(mode=True)\n",
    "    \n",
    "    with tqdm(total=batches) as progress_bar:\n",
    "        for batch_id in range(batches):\n",
    "            batch_start = batch_id * sequence_size\n",
    "\n",
    "            # reuse the hidden state from last batch\n",
    "            hidden_state = Variable(last_hidden_state)\n",
    "\n",
    "            # clear the gradient information from the past batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # for every symbol in the batch\n",
    "            # try predict the next symbol\n",
    "            # and meassure the loss\n",
    "            predictions = []\n",
    "            for sequence_id in range(sequence_size):\n",
    "                v_input_symbol = Variable(one_hot_encoding(data[batch_start + sequence_id]))\n",
    "\n",
    "                v_prediction, hidden_state = model(v_input_symbol, hidden_state)\n",
    "\n",
    "                predictions.append(v_prediction)\n",
    "\n",
    "            # create all labels\n",
    "            v_labels = Variable(labels_tensor(data[batch_start + 1:batch_start + sequence_size + 1]))\n",
    "\n",
    "            # create all predictions\n",
    "            v_predictions = torch.cat(predictions)\n",
    "\n",
    "            # backpropagate through time\n",
    "            v_loss = loss_function(v_predictions, v_labels)\n",
    "            v_loss.backward()\n",
    "\n",
    "            # gradient clipping to avoid exploding gradients\n",
    "            for parameter in model.parameters():\n",
    "                parameter.grad.data.clamp_(-gradient_clipping, gradient_clipping)\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # batch logging\n",
    "            loss = v_loss.data.item()\n",
    "            epoch_accumulated_loss += loss\n",
    "            \n",
    "            progress_bar.set_postfix(loss=\"{:.03f}\".format(loss))\n",
    "            progress_bar.update()\n",
    "\n",
    "            # take the hidden state out of the variable\n",
    "            # to avoid backpropagating the next batch to this one\n",
    "            last_hidden_state = hidden_state.data\n",
    "    \n",
    "    # epoch logging\n",
    "    mean_loss = epoch_accumulated_loss / float(batches)\n",
    "    print(\"Epoch {:d}/{:d} Mean Loss: {:.03f} Sample:\".format(epoch_id + 1, epochs, mean_loss))\n",
    "    print()\n",
    "    model.train(mode=False)\n",
    "    print_sample()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"models/min-char-rnn.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
