{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models Tiny ImageNet-200\n",
    "\n",
    "We are going to use the pre-trained models from [torchvision](http://pytorch.org/docs/master/torchvision/models.html) with the [Tiny ImageNet-200](https://tiny-imagenet.herokuapp.com/), a subset of ImageNet with 200 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some constants for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/tiny-imagenet-200/\"\n",
    "num_classes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and pre-processing\n",
    "\n",
    "First we load and pre-process the data according to the pre-trained model [documentation](http://pytorch.org/docs/master/torchvision/models.html), applying transformations using [this example](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L113).\n",
    "\n",
    "The training data does not really matter, we are only going to use the loader for the labels.\n",
    "\n",
    "For the validation data we use less corrupted images, only resizing them and cropping them in the center and then appliying the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modify this depending on memory constraints\n",
    "batch_size = 64\n",
    "\n",
    "# the magic normalization parameters come from the example\n",
    "transform_mean = np.array([ 0.485, 0.456, 0.406 ])\n",
    "transform_std = np.array([ 0.229, 0.224, 0.225 ])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = transform_mean, std = transform_std),\n",
    "])\n",
    "\n",
    "traindir = os.path.join(directory, \"train\")\n",
    "# be careful with this set, the labels are not defined using the directory structure\n",
    "valdir = os.path.join(directory, \"val\")\n",
    "\n",
    "train = datasets.ImageFolder(traindir, train_transform)\n",
    "val = datasets.ImageFolder(valdir, val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "assert num_classes == len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label madness\n",
    "\n",
    "All the pre-trained models from torchvision have the same output size: 1000. Even if they are popular classes from ImageNet, it is hard to get the labels. There is a map from integers in the interval \\[0, 999\\] (the one hot encoded ouput) to human readable strings that can be downloaded from [here](https://gist.github.com/rcamino/ad8e7853200096f5b8cbfaa15f4a77ea).\n",
    "\n",
    "We also build the inverse map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/ImageNet_1000_class_ids.json\", \"r\") as labels_file:\n",
    "    labels = json.load(labels_file)\n",
    "labels = dict([(int(k), v) for k, v in labels.items()])\n",
    "label_ids = dict([(v, k) for k, v in labels.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.items()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids.items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WordNet](https://wordnet.princeton.edu/) is a large lexical database of English. ImageNet uses a subset of this database as labels for the images, and the Tiny ImageNet-200 uses an even smaller subset. The Tiny ImageNet-200 comes with a map between WordNet ids and WordNet definitions in the file `words.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_labels = {}\n",
    "with open(os.path.join(directory, \"words.txt\"), \"r\") as dictionary_file:\n",
    "    line = dictionary_file.readline()\n",
    "    while line:\n",
    "        label_id, label = line.strip().split(\"\\t\")\n",
    "        small_labels[label_id] = label\n",
    "        line = dictionary_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_labels.items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train subdirectory of Tiny ImageNet-200 has a collection of subdirectories, named using to the WordNet ids to label the images that they contain. The torchvision data loader uses the names of the subdirectories as labels, but replaces them with numeric indices when iterating the batches.\n",
    "\n",
    "Note these indices are in the range \\[0, 199\\] and do not match the original indices in the interval \\[0, 999\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(traindir)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_label_map = {}\n",
    "for label_index, label_id in enumerate(train_loader.dataset.classes):\n",
    "    label = small_labels[label_id]\n",
    "    small_label_map[label_index] = label_ids[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_label_map.items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem is that the validation directory only has one subdirectory called `images`. The labels for every image inside this subdirectory are defined in a file called `val_annotations.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_map = {}\n",
    "with open(os.path.join(directory, \"val/val_annotations.txt\"), \"r\") as val_label_file:\n",
    "    line = val_label_file.readline()\n",
    "    while line:\n",
    "        file_name, label_id, _, _, _, _ = line.strip().split(\"\\t\")\n",
    "        val_label_map[file_name] = label_id\n",
    "        line = val_label_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_map.items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we update the Tiny ImageNet-200 validation set labels using the :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader.dataset.imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_loader.dataset.imgs)):\n",
    "    file_path = val_loader.dataset.imgs[i][0]\n",
    "    \n",
    "    file_name = os.path.basename(file_path)\n",
    "    label_id = val_label_map[file_name]\n",
    "    label = small_labels[label_id]\n",
    "    \n",
    "    val_loader.dataset.imgs[i] = (file_path, label_ids[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader.dataset.imgs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "We can chose any of the models from the [documentation](http://pytorch.org/docs/master/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet161(pretrained=True)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some samples\n",
    "\n",
    "To be sure that we did not mess up with the labels and see how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 10 # modify the number of images shown\n",
    "\n",
    "batch_inputs, batch_labels = next(iter(val_loader))\n",
    "batch_inputs = Variable(batch_inputs.cuda(), volatile=True)\n",
    "\n",
    "batch_logits = model(batch_inputs)\n",
    "\n",
    "batch_labels = batch_labels.numpy()\n",
    "batch_predictions = batch_logits.topk(5)[1].data.cpu().numpy()\n",
    "\n",
    "cell_number = 1\n",
    "\n",
    "plt.figure(figsize=(4, num_images * 2))\n",
    "\n",
    "for image_number in range(num_images):\n",
    "    image = np.copy(batch_inputs.data[image_number].cpu().numpy())\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    for channel in range(3):\n",
    "        image[:, :, channel] = image[:, :, channel] * transform_std[channel] + transform_mean[channel]\n",
    "\n",
    "    label = labels[batch_labels[image_number]]\n",
    "\n",
    "    plt.subplot(num_images, 2, cell_number)\n",
    "\n",
    "    ax = plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    cell_number += 1\n",
    "\n",
    "    plt.subplot(num_images, 2, cell_number)\n",
    "    plt.axis(\"off\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.text(0, 0.85, \"Label: {}\".format(label))\n",
    "    for prediction_number in range(5):\n",
    "        plt.text(0, 0.85 - 0.15 * (prediction_number + 1), \"Prediction-{:d}: {}\".format(\n",
    "            prediction_number + 1, labels[batch_predictions[image_number, prediction_number]]))\n",
    "    \n",
    "    cell_number += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We are going to meassure the Top-1 and Top-5 Error in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1 = 0.0\n",
    "top_5 = 0.0\n",
    "total = 0.0\n",
    "\n",
    "log_every = 10\n",
    "\n",
    "for batch_number, (batch_inputs, batch_labels) in enumerate(val_loader):\n",
    "    batch_inputs = Variable(batch_inputs.cuda(), volatile=True)\n",
    "    batch_logits = model(batch_inputs)\n",
    "    \n",
    "    batch_labels = batch_labels.numpy()\n",
    "    batch_predictions = batch_logits.data.topk(5)[1].cpu().numpy()\n",
    "            \n",
    "    total += len(batch_labels)\n",
    "    \n",
    "    for i in range(len(batch_labels)):\n",
    "        if batch_labels[i] == batch_predictions[i, 0]:\n",
    "            top_1 += 1\n",
    "            top_5 += 1\n",
    "        else:\n",
    "            for j in range(1, 5):\n",
    "                if batch_labels[i] == batch_predictions[i, j]:\n",
    "                    top_5 += 1\n",
    "                    break\n",
    "            \n",
    "    if batch_number % log_every == log_every - 1:\n",
    "        print(\"Batch {:3d} - Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "            batch_number + 1, 100 - top_1 / total * 100, 100 - top_5 / total * 100))\n",
    "        \n",
    "print(\"Top-1 Error: {:.3f} Top-5 Error: {:.3f}\".format(\n",
    "    100 - top_1 / total * 100, 100 - top_5 / total * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
