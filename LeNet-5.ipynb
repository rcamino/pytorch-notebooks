{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet\n",
    "\n",
    "![LeNet](images/lenet.png)\n",
    "\n",
    "Paper: [LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324.](http://yann.lecun.com/exdb/publis/psgz/lecun-98.ps.gz)\n",
    "\n",
    "Webpage: [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Torchvision has helpers to load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/mnist/raw\"\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [1.0])\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_path, train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(data_path, train=False, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5, 1, 2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5, 1, 0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, the weights are initialized with random values drawn from a uniform distribution between $-2.4 / F_i$ and $2.4 / F_i$ where $F_i$ is the number of input dimensions (fan-in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, a=0, mode=\"fan_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model = LeNet5()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batches(loader, title, train=True):\n",
    "    model.train(mode=train)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0.0\n",
    "    epoch_total = 0.0\n",
    "    with tqdm(total=len(loader)) as progress_bar:\n",
    "        progress_bar.set_description(title)\n",
    "        for batch_id, (images, labels) in enumerate(loader):\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            v_images = Variable(images)\n",
    "            v_labels = Variable(labels)\n",
    "\n",
    "            if use_cuda:\n",
    "                v_images = v_images.cuda()\n",
    "                v_labels = v_labels.cuda()\n",
    "\n",
    "            v_predictions = model(v_images)\n",
    "            v_loss = loss_function(v_predictions, v_labels)\n",
    "            v_correct = torch.eq(torch.max(v_predictions, 1)[1], v_labels).long().sum()\n",
    "\n",
    "            if use_cuda:\n",
    "                loss = v_loss.cpu().data.numpy()\n",
    "                correct = v_correct.cpu().data.numpy()\n",
    "            else:\n",
    "                loss = v_loss.data.numpy()\n",
    "                correct = v_correct.data.numpy()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_correct += correct\n",
    "            epoch_total += float(len(labels))\n",
    "\n",
    "            if train:\n",
    "                v_loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "            progress_bar.set_postfix(mean_loss=\"{:.03f}\".format(epoch_loss / epoch_total),\n",
    "                                     accuracy=\"{:.03f}\".format(epoch_correct / epoch_total))\n",
    "            progress_bar.update()\n",
    "            \n",
    "    return epoch_loss / epoch_total, epoch_correct / epoch_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch_id in range(epochs):\n",
    "    run_batches(train_loader, \"Train {}/{}\".format(epoch_id + 1, epochs), train=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        run_batches(test_loader, \"Test {}/{}\".format(epoch_id + 1, epochs), train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
